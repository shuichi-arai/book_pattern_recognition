----------------------------------------------------------------------------------------------------
第9章「サポートベクターマシン」

第9.5節で説明した逐次解法を実装しました．
具体的な漸化式は本文中p.209に(Program)と書いてあります．
----------------------------------------------------------------------------------------------------

SVM.py  --- 9.5節に示した逐次解法のサンプル

実行すると，逐次的に識別境界面が決まり，パターンの点の大きさが変化します．
パターンが「大きな四角い点」で表示されているものがサポートベクトルです．
学習の過程では，サポートベクトルの数や，どのパターンがサポートベクトルになるのかが変化し，
最終的にサポートベクトルが決まっていく様子が確認できます．

コマンドライン上には， (p.209)で説明しているベクトルaを表示しています．
ここが正値であればサポートベクトルであるということでしたね．
その値の変化を見ていると，「そろそろサポートベクトルから外れそう」な弱いサポートベクトルと，
「間違いなくこれはサポートベクトルになるな」という強いサポートベクトルがあるのが分かるでしょう．
まだ，学習が途中なのか否かも，このaの変化を見ているとわかります．

このプログラムはハードマージンとソフトマージンの両方に対応しています．(p.229)


データはランダムに生成します．
    (1) 生成する個数は "-n TRAIN_NUM" で指定します． defaultでは各クラス10データで計20個です．
    (2) 2クラス間のオーバーラップ量を "-o OVERWRAP" で指定できます．defaultでは0.0になっています．
        例えば オーバラップ量が0.0なら2クラスのデータは重ならず線形分離できます．hard-margin用ですね．
        オーバーラップ量が1.0だと100%重なります．
        soft-marginの実験用にデータを作成するときには10%から30%程度を指定すると良いかもしれません．
    (3) 乱数の種を指定すると同じデータを生成できます．できるだけ指定したほうが良いでしょう．
        "-s SEED"で指定できます．自然数を指定します．
	(4) SEEDを変えると異なるデータを生成しますが，本来なら2クラス分類に必要なサポートベクトルは2つのはずなのに
		それ以上の場合が生じます．その理由を実験的に考察してみるのも良いでしょう．

学習について
    (1) 最急降下法を何回回すかは"-i ITER_NUM"で指定できます．defaultでは 10000回になっています．
    (2) 学習率は "-e ETA"で指定できます．defaultでは0.1になっていますが，これ以上大きくすると発散するかもしれません．
    (3) soft-marginで学習するなら "-C SOFTC"を指定します．defaultでは1.e10と大きな値になっていて，ハートマージンSVMとして動作します．
        これをC=0.5とかにするとソフトマージンSVMとして動作します．本書(9.8節)に詳しい解説があります．
        



usage: SVM.py [-h] [-n TRAIN_NUM] [-o OVERWRAP] [-i ITER_NUM] [-s SEED]
              [-e ETA] [-C SOFTC]

optional arguments:
  -h, --help            show this help message and exit
  -n TRAIN_NUM, --train_num TRAIN_NUM
                        number of training data (default=20)
  -o OVERWRAP, --overwrap OVERWRAP
                        overwrap rate of 2 class data (default=0.0[no overwrap])
  -i ITER_NUM, --iter_num ITER_NUM
                        itteration number for training (default=10000)
  -s SEED, --seed SEED  random seed number (default:Random)
  -e ETA, --eta ETA     eta (default=0.1)
  -C SOFTC, --softC SOFTC
                        soft margin C (typically, C=0.5) (default C=1.e10
                        means Hard Margin)


とりあえず，
    ./SVM.py -s 1
と乱数の種だけを指定して動かしてみると，
サポートベクトルが変化していくのが表示されます．
グラフ上のデータ点が小さく表示されるとサポートベクトルではなくなります．
それは端末上に表示されるαの値が0になっていることからも分かります．
最初のうちはaは値を持っていますが，どんどん0になっていきサポートベクトルが限定されていく様子が見れるでしょう．
このようにSVMは学習されるのです．
